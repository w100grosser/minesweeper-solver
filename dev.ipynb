{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "\n",
    "from scipy.ndimage import convolve\n",
    "\n",
    "import time\n",
    "\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '/path/to/chromedriver' with the actual path to the chromedriver executable\n",
    "driver = webdriver.Chrome('chromedriver')\n",
    "url = 'https://minesweeperonline.com/#beginner'  # minesweeper beginner\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Minesweeper():\n",
    "    def __init__(self, driver):\n",
    "        self.driver = driver\n",
    "        self.reset()\n",
    "\n",
    "        \n",
    "        \n",
    "        self.end = 0\n",
    "        self.win = 0\n",
    "        self.lose = 0\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def update_action(self, action):\n",
    "        state = self.cell_state\n",
    "        reward = 0\n",
    "        self.action = np.zeros((81))\n",
    "        self.action[action] = 1\n",
    "        # self.action = np.reshape(action, (9,9))\n",
    "\n",
    "        cell_ij = np.unravel_index(action, (9, 9))\n",
    "        \n",
    "        \n",
    "        reward_array_add = np.argmax(state, axis=-1,keepdims=True)[:,:,0]\n",
    "\n",
    "        kernel = np.array([[1, 1, 1],\n",
    "                    [1, 0, 1],\n",
    "                    [1, 1, 1]])\n",
    "\n",
    "\n",
    "        surrounding_sum = convolve(reward_array_add, kernel, mode='constant', cval=0.0)\n",
    "\n",
    "        not_surrounded = (surrounding_sum % 9 == 0) * (surrounding_sum >= 27) * -10\n",
    "\n",
    "        reward_array_add = np.where(reward_array_add == 9, 10, -10)\n",
    "\n",
    "        self.reward_array = reward_array_add + not_surrounded\n",
    "        \n",
    "        self.reward_array = np.reshape(self.reward_array, (9,9))\n",
    "        # print(state.shape, cell_ij)\n",
    "        blocked = state[cell_ij[0], cell_ij[1],9] == 1\n",
    "\n",
    "        element = self.driver.find_element_by_id(f\"{cell_ij[0] + 1}_{cell_ij[1] + 1}\")\n",
    "\n",
    "        # Click on the element\n",
    "        element.click()\n",
    "        self.update()\n",
    "\n",
    "        if self.end:\n",
    "            if self.win:\n",
    "                self.reward_array[cell_ij[0], cell_ij[1]] += 25\n",
    "                reward = 50\n",
    "            else:\n",
    "                self.reward_array[cell_ij[0], cell_ij[1]] -= 15\n",
    "                reward = -20\n",
    "        elif blocked:\n",
    "            if not_surrounded[cell_ij[0], cell_ij[1]] == 0:\n",
    "                self.reward_array[cell_ij[0], cell_ij[1]] += 5\n",
    "                reward = 5\n",
    "            else:\n",
    "                self.reward_array[cell_ij[0], cell_ij[1]] += 0\n",
    "                reward = 0\n",
    "\n",
    "            not_surrounded\n",
    "        else:\n",
    "            self.reward_array[cell_ij[0], cell_ij[1]] = -20\n",
    "            reward = -5\n",
    "\n",
    "            \n",
    "        return self.cell_state[:,:,:], reward, self.reward_array, self.end, self.win, self.lose\n",
    "    \n",
    "    \n",
    "    def update(self):\n",
    "        try:\n",
    "            html_content = self.driver.page_source\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            quit()\n",
    "\n",
    "        self.soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        self.game = self.soup.find(\"html\").find(\"body\").find(\"table\").find(\"tbody\").find(\"tr\").find(\"td\").find(\"div\").find(\"div\", id=\"center-column\").find('div', id= \"game-container\").find('div', id = 'game')\n",
    "        self.game1 = self.game.find_all(class_=\"square\")\n",
    "        self.end = 0 if self.game.find(\"div\", class_=\"facesmile\") else 1\n",
    "        self.win = 1 if self.game.find(\"div\", class_=\"facewin\") else 0\n",
    "        self.lose = 1 if self.game.find(\"div\", class_=\"facedead\") else 0\n",
    "        self.cell_state[:,:,:] = 0\n",
    "        # time0 = time.time()\n",
    "        for i in range(1, 10):\n",
    "            for j in range(1, 10):\n",
    "                state = self.game1[(i - 1) * 9+j - 1][\"class\"][-1]\n",
    "                match state:\n",
    "                    case \"blank\":\n",
    "                        self.cell_state[i-1,j-1,9] = 1\n",
    "                    case value if value.startswith(\"open\"):\n",
    "                        self.cell_state[i-1,j-1, int(value[-1])] = 1\n",
    "                    case value if value.startswith(\"bombflagged\"):\n",
    "                        self.cell_state[i-1,j-1,10] = 1\n",
    "                    case _:\n",
    "                        return 1\n",
    "\n",
    "        # /html/body/table/tbody/tr/td/div/div[2]/div[1]/div[2]/div[1]\n",
    "        # facewin\n",
    "        # facesmile\n",
    "        # facedead\n",
    "        # print(time.time() - time0)\n",
    "\n",
    "\n",
    "\n",
    "        return 1\n",
    "    \n",
    "    def reset(self):\n",
    "        # Find the element by ID\n",
    "        element = self.driver.find_element_by_id(f\"face\")  \n",
    "\n",
    "        # Click on the element\n",
    "        element.click()\n",
    "\n",
    "        self.cell_state = np.zeros((9,9,11), dtype=np.float32)\n",
    "        self.update()\n",
    "\n",
    "\n",
    "\n",
    "        action = np.zeros(81, dtype=np.float32)\n",
    "        action[np.random.randint(81)] = 1\n",
    "        self.action = np.reshape(action, (9,9))\n",
    "        cell_ij = np.unravel_index(np.argmax(self.action), self.action.shape)\n",
    "        element = self.driver.find_element_by_id(f\"{cell_ij[0] + 1}_{cell_ij[1] + 1}\") \n",
    "\n",
    "        # Click on the element\n",
    "        element.click()\n",
    "        self.update()\n",
    "\n",
    "\n",
    "        self.reward_array = np.argmax(self.cell_state, axis=-1,keepdims=True)\n",
    "\n",
    "        self.reward_array = np.where(self.reward_array == 9, 2, -10)\n",
    "\n",
    "        # self.update_action(action, self.cell_state)\n",
    "\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dqn_model(input_size, output_size):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, input_dim=input_size, activation='relu'))\n",
    "    model.add(Dense(256, activation='tanh'))\n",
    "    model.add(Dense(output_size, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=Adam(lr = 0.001))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, gamma, epsilon, epsilon_decay, epsilon_min, model):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=100)\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        if model is None:\n",
    "            self.model = create_dqn_model(state_size, action_size)\n",
    "        else:\n",
    "            self.model = model\n",
    "\n",
    "    def remember(self, state, action, reward, reward_array, next_state, end):\n",
    "        self.memory.append((state, action, reward, reward_array, next_state, end))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.randint(self.action_size)\n",
    "        q_values = self.model.predict(state, verbose=0)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        # print(type(self.memory))\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, reward_array, next_state, end in minibatch:\n",
    "            target = reward\n",
    "            \n",
    "            if not end:\n",
    "                res = self.model.predict(next_state, verbose=0)[0]\n",
    "                # print(res)\n",
    "                \n",
    "                # target = (reward + self.gamma *\n",
    "                #           np.amax(res))\n",
    "                target = (reward)\n",
    "                # print(np.amax(res), reward)\n",
    "                \n",
    "            \n",
    "            reward_add = np.argmax(np.reshape(state, [9, 9, 11]), axis=-1,keepdims=True)\n",
    "\n",
    "            reward_add = np.where(reward_add == 0, 1, 0)\n",
    "                \n",
    "            reward_add = reward_add.flatten()\n",
    "\n",
    "            target_f = self.model.predict(state, verbose=0)\n",
    "            target_f[0][reward_add] = -10\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    simulator = Minesweeper(driver)\n",
    "    \n",
    "    agent = DQNAgent(9*9*11, 9*9, 0.95, 0.9, 0.95, 0.15, None)\n",
    "\n",
    "    end = False\n",
    "\n",
    "    batch_size = 16\n",
    "\n",
    "    episodes = 200\n",
    "\n",
    "    replay_counter = 0\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        simulator.reset()\n",
    "        state = simulator.cell_state\n",
    "        state = np.reshape(state, [1, 9 * 9* 11])\n",
    "        for epoch in range(30):\n",
    "            replay_counter += 1\n",
    "            # simulator.render()\n",
    "            action = agent.act(state)\n",
    "            # print(simulator.step(action))\n",
    "            # self.cell_state[:,:,:], reward, self.reward_array, self.end, self.win, self.lose\n",
    "            next_state, reward, reward_array, end, win, lose = simulator.update_action(action)\n",
    "            # print(action, reward)\n",
    "            # time.sleep(3)\n",
    "            # print(reward)\n",
    "            reward = reward if not end else -10\n",
    "            next_state = np.reshape(next_state, [1, (9*9*11)])\n",
    "            agent.remember(state, action, reward, reward_array, next_state, end)\n",
    "            state = next_state\n",
    "            if end:\n",
    "                print(f\"Episode: {episode} out of {episodes}\\tEpoch: {epoch}\\tEpsilon: {agent.epsilon:.2}\\tTraining iters: {replay_counter//20}\")\n",
    "                break\n",
    "            if len(agent.memory) > batch_size and replay_counter % 20 == 0:\n",
    "                agent.replay(batch_size)\n",
    "        model_save = agent.model\n",
    "        if episode % 10 == 0:\n",
    "            model_save.save(\"minesweeper_dqn_model.h5\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_save  = tf.keras.models.load_model('minesweeper_dqn_model.h5')\n",
    "\n",
    "gamer = Minesweeper(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamer.update()\n",
    "res = model_save.predict(np.reshape(gamer.cell_state.flatten(), (1,-1)))[0]\n",
    "index = np.argmax(res)\n",
    "print(np.unravel_index(index, (9,9)))\n",
    "\n",
    "\n",
    "# # print(reward_array.shape)\n",
    "# print('\\n'.join([''.join([f'{item:4}' for item in row]) \n",
    "#       for row in gamer.reward_array[:,:]]))\n",
    "\n",
    "# print(reward_array.shape)\n",
    "print('\\n'.join(['   '.join([f'{item:.1f}' for item in row]) \n",
    "      for row in np.reshape(res,(9,9))]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state, reward, reward_array, end, win, lose = gamer.update_action(index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
